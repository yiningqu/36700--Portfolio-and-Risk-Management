{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from functools import partial\n",
    "import sys\n",
    "import math\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb93573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "df = pd.read_excel('/Users/yiningqu/Desktop/dfa_analysis_data.xlsx', sheet_name = 'factors').set_index('Date')\n",
    "\n",
    "\n",
    "#import data\n",
    "raw_data = pd.read_excel('../data/momentum_data.xlsx',sheet_name = None)\n",
    "\n",
    "#import data with multiple sheets\n",
    "sheets = list(raw_data.keys())\n",
    "ff_factors = raw_data[sheets[1]].set_index('Date')\n",
    "momentum = raw_data[sheets[2]].set_index('Date')\n",
    "mom_deciles = raw_data[sheets[3]].set_index('Date')\n",
    "tercile_port = raw_data[sheets[4]].set_index('Date')\n",
    "rf = raw_data[sheets[5]].set_index('Date')\n",
    "ff_factors['UMD'] = momentum['UMD']\n",
    "\n",
    "\n",
    "#import data with skipped rows\n",
    "ltcm = pd.read_excel('../data/ltcm_exhibits_data.xlsx', sheet_name='Exhibit 2', skiprows=2, index_col=0, parse_dates=[0]).iloc[:-4, :]\n",
    "ltcm.index.name = 'Date'\n",
    "\n",
    "#import data with no 'Date'\n",
    "spy = pd.read_excel('../data/gmo_analysis_data.xlsx', sheet_name='returns (total)', index_col=0, parse_dates=[0])[['SPY']]\n",
    "us3m = pd.read_excel('../data/gmo_analysis_data.xlsx', sheet_name='risk-free rate', index_col=0, parse_dates=[0])[['US3M']]\n",
    "\n",
    "\n",
    "#datetime: format date\n",
    "ltcm.index = pd.to_datetime(ltcm.index, format='%Y-%m-%d') + pd.tseries.offsets.MonthEnd(0)\n",
    "\n",
    "\n",
    "#drop data\n",
    "factors = df.drop(['RF'], axis=1)\n",
    "\n",
    "\n",
    "#subtract\n",
    "portfolio_ret = portfolio.sub(risk_free.values).loc['1981':]\n",
    "df.loc[:, ['LTCM Net', 'LTCM Gross', 'SPY']] = df.loc[:, ['LTCM Net', 'LTCM Gross', 'SPY']].subtract(df['US3M'], axis=0)\n",
    "\n",
    "#rename\n",
    "ts = ts.rename(columns={'Market beta':'MKT','Value beta':'HML','Momentum beta':'UMD'})\n",
    "\n",
    "#log \n",
    "#log(1+x)\n",
    "for col in risk_free_rates.columns:\n",
    "    risk_free_rates[col] = risk_free_rates[col]\n",
    "    risk_free_rates['log_'+col] = np.log(1+risk_free_rates[col])\n",
    "#log(x)\n",
    "for col in fx_rates.columns:\n",
    "    fx_rates['log_'+ col] = fx_rates[col].apply(lambda x: np.log(x))\n",
    "\n",
    "for col in fx_rates.columns:\n",
    "    fx_rates['log_'+col] = np.log(fx_rates[col])\n",
    "\n",
    "#合并combine\n",
    "strat_var= pd.concat([strat_summary_df.loc[:,['VaR (0.05)']],\n",
    "                      market_summary.loc[:,['VaR (0.05)']],\n",
    "                      gmo_summary.loc[:,['VaR (0.05)']]])\n",
    "\n",
    "oos_r2_sum = pd.concat([OOS_r2_dp,OOS_r2_ep,OOS_r2_epdp,OOS_r2_all])\n",
    "    \n",
    "#join\n",
    "#join three dataset\n",
    "df = ltcm.join(spy, how='inner').join(us3m, how='inner') \n",
    "\n",
    "#join two\n",
    "pre_case = ta.calc_performance_metrics(factors.loc[:'2014']).iloc[:, :3]\n",
    "post_case = ta.calc_performance_metrics(factors.loc['2015':]).iloc[:, :3]\n",
    "pre_post = pre_case.join(post_case, lsuffix=' Pre', rsuffix=' Post')\n",
    "\n",
    "\n",
    "#generate call & put option\n",
    "#np.where(condition, true, false)\n",
    "spy_er_ltcm['SPY_call'] = np.where(spy_er_ltcm['SPY'] > 0.03, spy_er_ltcm['SPY'] - 0.03, 0)\n",
    "spy_er_ltcm['SPY_put'] = np.where(-0.03 - spy_er_ltcm['SPY'] >= 0, -0.03 - spy_er_ltcm['SPY'], 0)\n",
    "\n",
    "#display max & min\n",
    "print(\"The asset with the best Sharpe ratio is: \")\n",
    "display(summary_stats_excess_returns[summary_stats_excess_returns['Sharpe'] == summary_stats_excess_returns['Sharpe'].max()][['Sharpe']])\n",
    "print(\"The asset with the worst Sharpe ratio is: \")\n",
    "display(summary_stats_excess_returns[summary_stats_excess_returns['Sharpe'] == summary_stats_excess_returns['Sharpe'].min()][['Sharpe']])\n",
    "\n",
    "#find min & max corr \n",
    "correlation_matrix = excess_returns.corr()\n",
    "corr_rank = correlation_matrix.unstack().sort_values().to_frame('Correlations')\n",
    "corr_rank = corr_rank[corr_rank['Correlations']!=1]\n",
    "pair_max = corr_rank.index[-1]\n",
    "pair_min = corr_rank.index[0]\n",
    "\n",
    "\n",
    "#calculate frequency (excess r < r_var(0.05))\n",
    "(df['Excess SPY Returns'] < Historic_VaR).sum()/len(Historic_VaR.dropna())*100:.2f\n",
    "\n",
    "#calculate rolling frequency \n",
    "var_temp = VaR.dropna()\n",
    "frequency =  var_temp.apply(lambda x: rets.loc[var_temp.index] < x).mean().to_frame('frequency')\n",
    "frequency.style.format('{:,.2%}')\n",
    "\n",
    "#correlation\n",
    "summary['Corr Mkt'] = np.corrcoef(np.array(sub_factors['MKT']), np.array(sub_mom[port]))[0,1]\n",
    "\n",
    "#construct dataframe:\n",
    "dp_forecast = pd.DataFrame([[beta_dp,alpha,r_squared]],columns=['DP Beta','Alpha','R-Squared'],index = ['DP'])\n",
    "stats_list = pd.DataFrame(columns=['Mean', 'Volatility', 'Sharpe_ratio'], index=hedge_fund.columns)\n",
    "\n",
    "#St+1 - St\n",
    "strat = fx_rates[v].diff()\n",
    "\n",
    "#merge\n",
    "strat_rtn = strat_rtn.merge(forecasting[['GLD']], left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8763f",
   "metadata": {},
   "source": [
    "# Performance_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_summary(return_data, period = 12):\n",
    "    \"\"\" \n",
    "        Returns the Performance Stats for given set of returns\n",
    "        Inputs: \n",
    "            return_data - DataFrame with Date index and Monthly Returns for different assets/strategies.\n",
    "        Output:\n",
    "            summary_stats - DataFrame with annualized mean return, vol, sharpe ratio. Skewness, Excess Kurtosis, Var (0.5) and\n",
    "                            CVaR (0.5) and drawdown based on monthly returns. \n",
    "    \"\"\"\n",
    "    summary_stats = return_data.mean().to_frame('Mean').apply(lambda x: x*period)\n",
    "    summary_stats['Volatility'] = return_data.std().apply(lambda x: x*np.sqrt(period))\n",
    "    summary_stats['Sharpe Ratio'] = summary_stats['Mean']/summary_stats['Volatility']\n",
    "    summary_stats['Skewness'] = return_data.skew()\n",
    "    summary_stats['Excess Kurtosis'] = return_data.kurtosis()\n",
    "    summary_stats['VaR (0.05)'] = return_data.quantile(.05, axis = 0)\n",
    "    summary_stats['CVaR (0.05)'] = return_data[return_data <= return_data.quantile(.05, axis = 0)].mean()\n",
    "    summary_stats['Min'] = return_data.min()\n",
    "    summary_stats['Max'] = return_data.max()\n",
    "    \n",
    "    wealth_index = 1000*(1+return_data).cumprod()\n",
    "    previous_peaks = wealth_index.cummax()\n",
    "    drawdowns = (wealth_index - previous_peaks)/previous_peaks\n",
    "\n",
    "    summary_stats['Max Drawdown'] = drawdowns.min()\n",
    "    summary_stats['Peak'] = [previous_peaks[col][:drawdowns[col].idxmin()].idxmax() for col in previous_peaks.columns]\n",
    "    summary_stats['Bottom'] = drawdowns.idxmin()\n",
    "    \n",
    "    recovery_date = []\n",
    "    for col in wealth_index.columns:\n",
    "        prev_max = previous_peaks[col][:drawdowns[col].idxmin()].max()\n",
    "        recovery_wealth = pd.DataFrame([wealth_index[col][drawdowns[col].idxmin():]]).T\n",
    "        recovery_date.append(recovery_wealth[recovery_wealth[col] >= prev_max].index.min())\n",
    "    summary_stats['Recovery'] = recovery_date\n",
    "    \n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fda44",
   "metadata": {},
   "source": [
    "### two or more sample\n",
    "#### Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_1980 = factors.loc[:'1980']\n",
    "sub_2001 = factors.loc['1981':'2001']\n",
    "sub_2023 = factors.loc['2002':]\n",
    "\n",
    "df_dict={'1926-1980' : sub_1980,\n",
    "         '1981-2001' : sub_2001,\n",
    "         '2002-2023' : sub_2023}\n",
    "\n",
    "summary_lst = []\n",
    "for key in df_dict.keys():\n",
    "    summary_stats = performance_summary(df_dict[key], period = 12).loc[:,['Mean','Volatility','Sharpe Ratio','VaR (0.05)']]\n",
    "    summary_stats['Period'] = key\n",
    "    summary_stats= summary_stats.reset_index().rename(columns = {'index':'Factor'}).set_index(['Period','Factor'])\n",
    "    summary_lst.append(summary_stats)\n",
    "\n",
    "factor_summary = pd.concat(summary_lst)\n",
    "factor_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将exchange rate和risk free rate两个dataset的column name 合并\n",
    "fx_spot_map = {'log_GBP1M':'log_USUK'\n",
    "                ,'log_EUR1M':'log_USEU'\n",
    "                ,'log_CHF1M':'log_USSZ'\n",
    "                ,'log_JPY1M':'log_USJP'\n",
    "}\n",
    "\n",
    "fx_hldg_lst = []\n",
    "for k,v in fx_spot_map.items():\n",
    "    #fx_rate shift(1) = St+1; fx_rate = St; risk_free shift(1) = Rt+1\n",
    "    fx_hldg_excess_ret = fx_rates[v] - fx_rates[v].shift(1) + risk_free_rates[k].shift(1) - risk_free_rates['log_USD1M'].shift(1)\n",
    "    fx_hldg_summary = ta.performance_summary(fx_hldg_excess_ret.to_frame().dropna())\n",
    "    #change log_GBP1M to GBP\n",
    "    fx_hldg_summary.index = [k[4:7]]\n",
    "    fx_hldg_summary.index.name = 'Currency Held'\n",
    "    fx_hldg_lst.append(fx_hldg_summary)\n",
    "\n",
    "fx_hldg_perf_summary = pd.concat(fx_hldg_lst)\n",
    "fx_hldg_perf_summary.loc[:, ['Mean','Volatility','Sharpe Ratio', 'Min', 'Max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e931d63",
   "metadata": {},
   "source": [
    "# Tangency Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tangency_weights(returns, cov_mat = 1):\n",
    "    \n",
    "    if cov_mat ==1:\n",
    "        cov_inv = np.linalg.inv((returns.cov()*12))\n",
    "    else:\n",
    "        cov = returns.cov()\n",
    "        covmat_diag = np.diag(np.diag((cov)))\n",
    "        covmat = cov_mat * cov + (1-cov_mat) * covmat_diag\n",
    "        cov_inv = np.linalg.inv((covmat*12))  \n",
    "        \n",
    "    ones = np.ones(returns.columns[1:].shape) \n",
    "    mu = returns.mean()*12\n",
    "    scaling = 1/(np.transpose(ones) @ cov_inv @ mu)\n",
    "    tangent_return = scaling*(cov_inv @ mu) \n",
    "    tangency_wts = pd.DataFrame(index = returns.columns[1:], data = tangent_return, columns = ['Tangent Weights'] )\n",
    "        \n",
    "    return tangency_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95f77d",
   "metadata": {},
   "source": [
    "* Compute the mean, volatility, and Sharpe ratio for the tangency portfolio corresponding to $\\wtan$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tangency weight * excess return\n",
    "w_tan_summary = performance_summary(excess_returns @ w_t , annual_factor = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = .015\n",
    "excess_returns = multi_asset_etf_excess_ret[multi_asset_etf_excess_ret.columns[1:]] \n",
    "w_t_without_tips.loc['TIP'] = 0\n",
    "wts = pd.DataFrame(index = excess_returns.columns, columns = ['tangency','tangency with TIPS dropped',\n",
    "                                                              'tangency with TIPS adjusted','equal weights',\n",
    "                                                              'risk parity','regularized'])\n",
    "\n",
    "\n",
    "wts.loc[:,'tangency'] = tangency_weights(multi_asset_etf_excess_ret, cov_mat = 1).values\n",
    "wts.loc[:,'tangency with TIPS dropped'] = w_t_without_tips.values\n",
    "wts.loc[:,'tangency with TIPS adjusted'] = w_t_adj_tips.values\n",
    "wts.loc[:,'equal weights'] = 1/len(excess_returns)\n",
    "wts.loc[:,'risk parity'] = 1/excess_returns.var()\n",
    "#*********************************\n",
    "wts.loc[:,'regularized'] = tangency_weights(multi_asset_etf_excess_ret, cov_mat = 0.5).values\n",
    "\n",
    "#*********************************\n",
    "wts *= target_mean / (excess_returns.mean()@wts)\n",
    "\n",
    "wts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5953aca",
   "metadata": {},
   "source": [
    "# regression_based_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e30e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one factor\n",
    "def regression_based_performance(fund_ret,factor, rf,constant = True):\n",
    "    \"\"\" \n",
    "        Returns the Regression based performance Stats for given set of returns and factors\n",
    "        Inputs:\n",
    "            factor - Dataframe containing monthly returns of the regressors\n",
    "            fund_ret - Dataframe containing monthly excess returns of the regressand fund\n",
    "            rf - Monthly risk free rate of return\n",
    "        Output:\n",
    "            summary_stats - (Beta of regression, treynor ratio, information ratio, alpha). \n",
    "    \"\"\"\n",
    "    if constant:\n",
    "        X = sm.tools.add_constant(factor)\n",
    "    else:\n",
    "        X = factor\n",
    "    y=fund_ret\n",
    "    model = sm.OLS(y,X,missing='drop').fit()\n",
    "    \n",
    "    if constant:\n",
    "        beta = model.params[1:]\n",
    "        alpha = round(float(model.params['const']),6) *12\n",
    "\n",
    "        \n",
    "    else:\n",
    "        beta = model.params\n",
    "    treynor_ratio = ((fund_ret - rf).mean()*12)/beta[0]\n",
    "    tracking_error = (model.resid.std()*np.sqrt(12))\n",
    "    if constant:        \n",
    "        information_ratio = model.params[0]*12/tracking_error\n",
    "    r_squared = model.rsquared\n",
    "    if constant:\n",
    "        return (beta,treynor_ratio,information_ratio,alpha,r_squared,tracking_error,model.resid)\n",
    "    else:\n",
    "        return (beta,treynor_ratio,r_squared,tracking_error,model.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one fund one factor\n",
    "betas, treynor, alpha, r_squared, tracking_er_vol = regression_based_performance(\n",
    "    merrill, hfri, risk_free, multiple=True, alpha=False)\n",
    "coefficients_new = pd.DataFrame(betas, columns=['beta'], index=merrill.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d8ece",
   "metadata": {},
   "source": [
    "## Multiple sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple sample\n",
    "reg_sub_sample = []\n",
    "for k,v in sub_samples.items():    \n",
    "    y = gmo.loc[sub_samples[k][0]:sub_samples[k][1],['GMWAX']].dropna()\n",
    "    X = gmo.loc[y.index[0]:y.index[-1],['SPY']]\n",
    "    reg = regression_based_performance(X,y)\n",
    "    beta_mkt = reg[0][0]\n",
    "    alpha = reg[3]\n",
    "    r_squared = reg[4]\n",
    "    reg_sub_sample.append(pd.DataFrame([[beta_mkt,alpha,r_squared]],columns=['SPY Beta','Alpha','R-Squared'],index = [k]))\n",
    "\n",
    "reg_performance = pd.concat(reg_sub_sample)\n",
    "reg_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ca4ca",
   "metadata": {},
   "source": [
    "## Multiple factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89587e17",
   "metadata": {},
   "source": [
    "### method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple fund one factor\n",
    "regression_stats_df = pd.DataFrame(columns=[\n",
    "                                   'Market_Beta', 'Treynor_ratio', \n",
    "                                    'Information_ratio', 'Alpha', 'R_Squared'], index=hedge_fund.columns)\n",
    "risk_free = merrill['USGG3M Index']\n",
    "stats_list = pd.DataFrame(\n",
    "    columns=['Mean', 'Volatility', 'Sharpe_ratio'], index=hedge_fund.columns)\n",
    "\n",
    "for fund in hedge_fund:\n",
    "    beta, treynor, information, alpha, r_squared, _ = regression_based_performance(\n",
    "        hedge_fund[fund],merrill['SPY US Equity'], risk_free)\n",
    "    regression_stats_df.loc[fund] = [\n",
    "        beta[0], treynor, information, alpha, r_squared]\n",
    "\n",
    "    cur_stats = stats(hedge_fund[fund].to_frame())\n",
    "    stats_list.loc[fund] = [cur_stats['Mean'][0],\n",
    "                            cur_stats['Volatility'][0], cur_stats['Sharpe_ratio'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aeeea4",
   "metadata": {},
   "source": [
    "### method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y=change in fx, x=interest rate difference --> whether follows UIP\n",
    "fx_hldg_reg = []\n",
    "for k,v in fx_spot_map.items():\n",
    "    factor = risk_free_rates['log_USD1M'] - risk_free_rates[k]\n",
    "    #fx_rates[v].diff() = fx_rates[v] - fx_rates[v].shift(1)\n",
    "    strat = fx_rates[v].diff()\n",
    "    reg = regression_based_performance(factor,strat,0)\n",
    "    beta_currency = reg[0][0]\n",
    "    treynor_ratio = reg[1]\n",
    "    information_ratio = reg[2]\n",
    "    alpha = reg[3]\n",
    "    r_squared = reg[4]\n",
    "    fx_hldg_reg.append(pd.DataFrame([[alpha,beta_currency,r_squared]],columns=['Alpha','Beta','R-Squared'],index = [k[4:7]]))\n",
    "\n",
    "\n",
    "fx_hldg_reg_summary = pd.concat(fx_hldg_reg)\n",
    "fx_hldg_reg_summary = fx_hldg_reg_summary.T\n",
    "fx_hldg_reg_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = gmo_total[['SPY']]\n",
    "X = signals[['DP', 'EP','US10Y']].shift(1)\n",
    "reg = regression_based_performance(X,y)\n",
    "beta_dp = reg [0][0]\n",
    "beta_ep = reg[0][1]\n",
    "beta_rf = reg[0][2]\n",
    "alpha = reg[3]\n",
    "r_squared = reg[4]\n",
    "forecast = pd.DataFrame([[beta_dp,beta_ep,beta_rf,alpha,r_squared]],columns=['DP Beta','EP Beta','RF Beta','Alpha','R-Squared'],index = ['3_factors'])\n",
    "forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f4a13",
   "metadata": {},
   "source": [
    "# Time-series Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only one factor time series regression\n",
    "def time_series_regression(portfolio, factors, FF3F = False, resid = False, scale =12):\n",
    "    '''\n",
    "    Input portfolio (columns of funds to be regressed), and factors.\n",
    "    This function returns the portfolio columns as index, and regression estimates as columns\n",
    "    If factors contain Fama-French 3 Factors, (Size and Value), then includes those betas.\n",
    "    If resid = True, then we return the residual of each column in portfolio indexed by time\n",
    "    '''\n",
    "    \n",
    "    report = pd.DataFrame(index = portfolio.columns)\n",
    "    residual = pd.DataFrame(columns= portfolio.columns)\n",
    "\n",
    "    for col in portfolio.columns:\n",
    "        fund_ret = portfolio[col]\n",
    "        model = sm.OLS(fund_ret, sm.add_constant(factors), missing= 'drop').fit()\n",
    "        report.loc[col, 'Alpha'] = model.params['const'] * scale\n",
    "        report.loc[col, 'Market Beta'] = model.params[1]\n",
    "        if FF3F:\n",
    "            report.loc[col, 'Size Beta'] = model.params[2]\n",
    "            report.loc[col, 'Value Beta'] = model.params[3]\n",
    "        report.loc[col, 'Information Ratio'] = np.sqrt(scale) * model.params['const'] / model.resid.std()\n",
    "        report.loc[col, 'Treynor Ratio'] = scale * portfolio[col].mean() / model.params[1]\n",
    "        report.loc[col, 'R-Squared'] = model.rsquared\n",
    "        report.loc[col, 'Tracking Error'] = (model.resid.std()*np.sqrt(scale))  \n",
    "        if resid:\n",
    "            residual[col] = model.resid\n",
    "    if resid:\n",
    "        return residual\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6306d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple factor time series regression\n",
    "def time_series_regression(portfolio, factors, resid = False, scale =12):\n",
    "    '''\n",
    "    Input portfolio (columns of funds to be regressed), and factors.\n",
    "    This function returns the portfolio columns as index, and regression estimates as columns\n",
    "    Function includes betas for anything included in factors\n",
    "    If resid = True, then we return the residual of each column in portfolio indexed by time\n",
    "    '''\n",
    "    \n",
    "    report = pd.DataFrame(index = portfolio.columns)\n",
    "    residual = pd.DataFrame(columns= portfolio.columns)\n",
    "\n",
    "    for col in portfolio.columns:\n",
    "        fund_ret = portfolio[col]\n",
    "        model = sm.OLS(fund_ret, sm.add_constant(factors), missing= 'drop').fit()\n",
    "        report.loc[col, 'Mean Return'] = scale * fund_ret.mean()\n",
    "        report.loc[col, 'Sharpe Ratio'] = np.sqrt(scale) * fund_ret.mean() / fund_ret.std()\n",
    "        report.loc[col, 'Alpha'] = model.params['const'] * scale\n",
    "        for i, factor in enumerate(factors.columns):\n",
    "            report.loc[col, f'{factor} Beta'] = model.params[i + 1]\n",
    "        report.loc[col, 'Information Ratio'] = np.sqrt(scale) * model.params['const'] / model.resid.std()\n",
    "        report.loc[col, 'Treynor Ratio'] = scale * portfolio[col].mean() / model.params[1]\n",
    "        report.loc[col, 'R-Squared'] = model.rsquared\n",
    "        report.loc[col, 'Tracking Error'] = (model.resid.std()*np.sqrt(scale))  \n",
    "        if resid:\n",
    "            residual[col] = model.resid\n",
    "    if resid:\n",
    "        return residual\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c65b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple sample\n",
    "reg_sub_sample = []\n",
    "for k,v in sub_samples.items():    \n",
    "    y = gmo.loc[sub_samples[k][0]:sub_samples[k][1],['GMWAX']].dropna()\n",
    "    X = gmo.loc[y.index[0]:y.index[-1],['SPY']]\n",
    "    reg = regression_based_performance(X,y)\n",
    "    beta_mkt = reg[0][0]\n",
    "    alpha = reg[3]\n",
    "    r_squared = reg[4]\n",
    "    reg_sub_sample.append(pd.DataFrame([[beta_mkt,alpha,r_squared]],columns=['SPY Beta','Alpha','R-Squared'],index = [k]))\n",
    "\n",
    "reg_performance = pd.concat(reg_sub_sample)\n",
    "reg_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one factor\n",
    "factor = spy_er_ltcm['SPY']\n",
    "reg_sum = []\n",
    "for rets in ['Gross Excess Returns','Net Excess Returns']:\n",
    "    fund_ret = ltcm_returns.loc[:, [rets]]\n",
    "    reg = regression_based_performance(factor, fund_ret, 0)\n",
    "    beta_mkt = reg[0][0]\n",
    "    treynor_ratio = reg[1][0]\n",
    "    information_ratio = reg[2]\n",
    "    alpha = reg[3]\n",
    "    r_squared = reg[4]\n",
    "    reg_sum.append(pd.DataFrame([[alpha, beta_mkt, r_squared, treynor_ratio, information_ratio]], columns = ['Alpha', 'Market Beta','R-Squared','Treynor Ratio','Information Ratio'], index=[rets]))\n",
    "\n",
    "mkt_reg_sum = pd.concat(reg_sum)\n",
    "mkt_reg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd941ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two factors\n",
    "spy_er_ltcm['$SPY^{2}$'] = spy_er_ltcm['SPY']**2\n",
    "factor_squared = spy_er_ltcm.loc[:, ['SPY', '$SPY^{2}$']]\n",
    "quad_reg_sum = []\n",
    "for rets in ['Gross Excess Returns','Net Excess Returns']:\n",
    "    fund_ret = ltcm_returns.loc[:, [rets]]\n",
    "    reg_squared = regression_based_performance(factor_squared, fund_ret, 0)\n",
    "    #这里需要自己改：有几个beta就写几个beta\n",
    "    beta_mkt = reg_squared[0][0]\n",
    "    beta_mkt_squared = reg_squared[0][1]\n",
    "    #------------------------------------------\n",
    "    treynor_ratio = reg_squared[1][0]\n",
    "    information_ratio = reg_squared[2]\n",
    "    alpha = reg_squared[3]\n",
    "    r_squared = reg_squared[4]\n",
    "    #这里需要改beta的名称\n",
    "    quad_reg_sum.append(pd.DataFrame([[alpha, beta_mkt, beta_mkt_squared, r_squared, treynor_ratio, information_ratio]], columns = ['Alpha', 'Market Beta', '$SPY^{2}$ Beta', 'R-Squared','Treynor Ratio','Information Ratio'], index=[rets]))\n",
    "    #------------------------------------------\n",
    "\n",
    "mkt_quad_reg_sum = pd.concat(quad_reg_sum)\n",
    "mkt_quad_reg_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91c6a8",
   "metadata": {},
   "source": [
    "# Cross-sectional Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4936fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross sectional test:\n",
    "portfolio = portfolio_ret.mean().to_frame('Mean Portfolio excess returns')\n",
    "cs_CAPM = time_series_regression(portfolio, ts_CAPM['Market Beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b359e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tangent weight\n",
    "def tangency_portfolio_rfr(asset_return,cov_matrix):\n",
    "    \"\"\" \n",
    "        Returns the tangency portfolio weights in a (1 x n) vector when a riskless assset is available\n",
    "        Inputs: \n",
    "            asset_return - Excess return over the risk free rate for each asset (n x 1) Vector\n",
    "            cov_matrix = nxn covariance matrix for the assets\n",
    "    \"\"\"\n",
    "    asset_cov = np.array(cov_matrix)\n",
    "    inverted_cov= np.linalg.inv(asset_cov)\n",
    "    one_vector = np.ones(len(cov_matrix.index))\n",
    "    \n",
    "    den = (one_vector @ inverted_cov) @ (asset_return)\n",
    "    num =  inverted_cov @ asset_return\n",
    "    return (1/den) * num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tangency portfolio instruction\n",
    "mean_ret = factor_summary['Mean']\n",
    "tan_weights = tangency_portfolio_rfr(mean_ret, factors.cov())\n",
    "tan_weights = pd.DataFrame(tan_weights, index = factors.columns, columns = ['Tangent Weights'])\n",
    "\n",
    "#put mean and tangent weights together \n",
    "tan_mean_weights = pd.concat([tan_weights.T, factor_summary[['Mean']].T]).T\n",
    "tan_mean_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce60e2",
   "metadata": {},
   "source": [
    "# Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb298bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([(((ts_CAPM['alpha_hat']).abs().mean()))], columns = ['MAE (%)'], index = ['CAPM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b780cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.loc[:, summary.columns.str.contains('Alpha')].abs().mean().to_frame('MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c84625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(regression_result, scale = 12):\n",
    "    return round(abs(regression_result['Alpha']).mean(), 6)\n",
    "\n",
    "#Mean Absolute Value Instruction\n",
    "mae_df = pd.DataFrame({\n",
    "    'Model': ['CAPM', 'FF3', 'FF5', 'AQR'],\n",
    "    'MAE': [calculate_mae(CAPM_regression), calculate_mae(FF3_regression), calculate_mae(FF5_regression), calculate_mae(AQR_regression)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cs_regression(mean_data, beta_data, model_name):\n",
    "    def perform_cs_regression(mean_data, beta_data, model_name):\n",
    "        CS_result = sm.OLS(mean_data, beta_data, missing='drop').fit()\n",
    "        CS_MAE = abs(CS_result.resid).mean()\n",
    "        CS_premia = CS_result.params.to_frame(f'{model_name} Premia')\n",
    "        return CS_result, CS_MAE, CS_premia\n",
    "    CS_result, CS_MAE, premia = perform_cs_regression(mean_data, beta_data, model_name)\n",
    "    premia.rename(index=lambda x: x.replace(f' {model_name} Beta', f' {model_name}'), inplace=True)\n",
    "    return CS_result, CS_MAE, premia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452bcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一定要记得乘以12\n",
    "#如果直接用time_series regression就不需要\n",
    "portfolio = portfolios_ex.mean()*12.to_frame('Mean Portfolio excess returns')\n",
    "# Assuming 'factor_returns' is the DataFrame that contains the factor returns for MKT, HML, and UMD.\n",
    "cs_CAPM = time_series_regression(portfolio, ts_regression[['MKT Beta', 'HML Beta', 'UMD Beta']])\n",
    "cs_CAPM = cs_CAPM[['Alpha', 'MKT Beta Beta','HML Beta Beta', 'UMD Beta Beta', 'R-Squared' ]]\n",
    "cs_CAPM.rename(columns={'Alpha': 'annualized intercept', 'MKT Beta Beta': 'annualized market beta', 'HML Beta Beta': 'annualized value beta','UMD Beta Beta': 'annualized momentum beta','R-Squared':'R-Squared'}, inplace=True)\n",
    "cs_CAPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fbbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#premia 就是beta的值（就是mean是左边，右边是之前算出来的几个factor的beta值，用beta的值作为regressor来算beta --> premia\n",
    "#用在time series里面每个factor的mean和在cross sectional test里面的premia（beta的值）进行比较，接近说明好\n",
    "AQR_CS, AQR_CS_MAE, AQR_CS_premia = process_data_regression(AQR_mean, AQR_beta, 'AQR')\n",
    "FF3_CS, FF3_CS_MAE, FF3_CS_premia = process_data_regression(FF3_mean, FF3_beta, 'FF3')\n",
    "FF5_CS, FF5_CS_MAE, FF5_CS_premia = process_data_regression(FF5_mean, FF5_beta, 'FF5')\n",
    "\n",
    "cs_premia = pd.concat([mean_ex_ret, AQR_CS_premia, FF3_CS_premia, FF5_CS_premia], axis = 1).fillna('')\n",
    "display(cs_premia)\n",
    "\n",
    "\n",
    "cs_mae = pd.Series([AQR_CS_MAE, FF3_CS_MAE, FF5_CS_MAE], index = ['AQR', 'FF3', 'FF5'])\n",
    "mae_ts = mae_df.loc[['AQR', 'FF3', 'FF5']]\n",
    "mae = pd.concat([mae_ts, cs_mae], axis = 1)\n",
    "mae.columns = ['Time Series MAE', 'Cross Section MAE']\n",
    "display(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696ea7c",
   "metadata": {},
   "source": [
    "# OOS Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function \n",
    "\n",
    "def OOS_r2(df, X, window):\n",
    "    y = df['SPY']\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    forecast_err, null_err = [], []\n",
    "\n",
    "    # eumerate: for index,value in eumerate(iterable)\n",
    "    for i,j in enumerate(df.index):\n",
    "        if i >= window:\n",
    "            # for an expanding window\n",
    "            currX = X.iloc[:i]\n",
    "            currY = y.iloc[:i]\n",
    "\n",
    "            # apply OLS on X and Y\n",
    "            reg = sm.OLS(currY, currX, missing = 'drop').fit()\n",
    "\n",
    "            null_forecast = currY.mean()\n",
    "\n",
    "            # regression prediction y value using x value\n",
    "            reg_predict = reg.predict(X.iloc[[i]])\n",
    "\n",
    "            actual = y.iloc[[i]]\n",
    "\n",
    "            forecast_err.append(reg_predict - actual)\n",
    "            null_err.append(null_forecast - actual)\n",
    "            \n",
    "    RSS = (np.array(forecast_err)**2).sum()\n",
    "    TSS = (np.array(null_err)**2).sum()\n",
    "    \n",
    "    return ((1 - RSS/TSS),reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa08833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OOS_forecasting(df, factors, start):\n",
    "    y = df\n",
    "    X = sm.add_constant(factors)\n",
    "\n",
    "    forecast_err, null_err,oos_predictions,null_predictions = [], [],[],[]\n",
    "\n",
    "    for i,j in enumerate(df.index):\n",
    "        if i >= start:\n",
    "            currX = X.iloc[:i]\n",
    "            currY = y.iloc[:i]\n",
    "            reg = sm.OLS(currY, currX, missing = 'drop').fit()\n",
    "            null_forecast = currY.mean()\n",
    "            reg_predict = reg.predict(X.iloc[[i]])\n",
    "            actual = y.iloc[[i]]\n",
    "            oos_predictions.append(reg_predict.T)\n",
    "            null_predictions.append(pd.DataFrame([[reg_predict.index[0]]], columns = ['Date'], index = [null_forecast]))\n",
    "            forecast_err.append(reg_predict.values - actual)\n",
    "            null_err.append(null_forecast.values - actual)\n",
    "            \n",
    "    RSS = (np.array(forecast_err)**2).sum()\n",
    "    TSS = (np.array(null_err)**2).sum()\n",
    "    predictions_df = pd.concat(oos_predictions).T.drop_duplicates()\n",
    "    null_predictions_df = pd.concat(null_predictions).T\n",
    "    \n",
    "    return ((1 - RSS/TSS),reg,predictions_df,null_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994322d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = signals.loc[:,'EP'].shift(1).to_frame()\n",
    "fund_ret = gmo_total.loc[factor.index[0]:,['SPY']]\n",
    "reg_ep = OOS_r2(fund_ret,factor,60)\n",
    "OOS_RSquared_ep = reg_ep[0]\n",
    "OOS_r2_ep = pd.DataFrame([[OOS_RSquared_ep]], columns = ['OOS R-Squared'], index = ['EP'])\n",
    "reg_ep_params = reg_ep[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6c286",
   "metadata": {},
   "source": [
    "# OOS Dynamic Forecast Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6afb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "\n",
    "def OOS_strat(df, factors, start, weight):\n",
    "    returns = []\n",
    "    y = df['SPY']\n",
    "    X = sm.add_constant(factors)\n",
    "\n",
    "    for i,j in enumerate(df.index):\n",
    "        if i >= start:\n",
    "            # expanding window\n",
    "            currX = X.iloc[:i]\n",
    "            currY = y.iloc[:i]\n",
    "            reg = sm.OLS(currY, currX, missing = 'drop').fit()\n",
    "            pred = reg.predict(X.iloc[[i]])\n",
    "            w = pred * weight\n",
    "            returns.append((df.iloc[i]['SPY'] * w)[0])\n",
    "\n",
    "    df_strat = pd.DataFrame(data = returns, index = df.iloc[-(len(returns)):].index, columns = ['Strat Returns'])\n",
    "    return df_strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac71435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create return rt+1 for SPY\n",
    "factor = signals.loc[:,'EP'].shift(1).to_frame()\n",
    "fund_ret= gmo_total.loc[factor.index[0]:,['SPY']]\n",
    "OOS_EP_predict = OOS_strat(fund_ret,factor, 60, 100).rename(columns={'Strat Returns':'EP_OOS_Returns'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat\n",
    "oos_prediction_sum = pd.concat([OOS_DP_predict.T,OOS_EP_predict.T,OOS_EPDP_predict.T,OOS_all_predict.T])\n",
    "oos_prediction_sum = oos_prediction_sum.T\n",
    "oos_prediction_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regress: fund_ret = rt+1(the prediction you calculated), factor = rt \n",
    "oos_prediction_sum = pd.concat([OOS_DP_predict.T,OOS_EP_predict.T,OOS_all_predict.T])\n",
    "oos_prediction_sum = oos_prediction_sum.T\n",
    "\n",
    "strats = {'DP': OOS_DP_predict.dropna(),\n",
    "          'EP': OOS_EP_predict.dropna(),\n",
    "          'DP-EP':OOS_EPDP_predict.dropna(),\n",
    "          'All': OOS_all_predict.dropna(),\n",
    "          'SPY':gmo_excess_ret.loc[OOS_all_predict.index[0]:,['SPY']].rename(columns={'SPY':'SPY_OOS_Returns'}),\n",
    "          'US3M':rf['US3M'].to_frame('US3M_OOS_Returns')\n",
    "         }\n",
    "factor = gmo_excess_ret.loc[:,['SPY']]\n",
    "strat_summary =[]\n",
    "for k,v in strats.items():\n",
    "    strat = strats[k]\n",
    "    perf_summary = performance_summary(strat)\n",
    "    perf_summary['Negative Risk Premium Months'] = len(strat[strat[k+'_OOS_Returns'] - rf['US3M'] <0])\n",
    "    perf_summary['Total Months'] = len(strat)\n",
    "    perf_summary.index = [k]\n",
    "    reg = time_series_regression(strat, factor[strat.index[0]:].squeeze(), False)\n",
    "    perf_summary['Market Beta'] = reg['SPY beta'].values\n",
    "    perf_summary['Market Alpha'] = reg['alpha_hat'].values\n",
    "    perf_summary['Market Information Ratio'] = reg['info_ratio'].values\n",
    "    strat_summary.append(perf_summary)\n",
    "    \n",
    "\n",
    "strat_summary_df = pd.concat(strat_summary)\n",
    "strat_summary_df.loc[:,['Mean','Volatility','Sharpe Ratio','VaR (0.05)','Max Drawdown','Market Beta','Market Alpha','Market Information Ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#带年份：\n",
    "oos_prediction_sum = pd.concat([OOS_DP_predict.T,OOS_EP_predict.T,OOS_all_predict.T])\n",
    "oos_prediction_sum = oos_prediction_sum.T\n",
    "\n",
    "strats = {'DP': OOS_DP_predict.dropna(),\n",
    "          'EP': OOS_EP_predict.dropna(),\n",
    "          'DP-EP':OOS_EPDP_predict.dropna(),\n",
    "          'All': OOS_all_predict.dropna(),\n",
    "          'US3M':rf['US3M'].to_frame('US3M_OOS_Returns')\n",
    "         }\n",
    "factor = gmo_excess_ret.loc[:,['SPY']]['2000':'2011']\n",
    "strat_summary =[]\n",
    "for k,v in strats.items():\n",
    "    strat = strats[k]['2000':'2011']\n",
    "    perf_summary = performance_summary(strat)\n",
    "    perf_summary['Negative Risk Premium Months'] = len(strat[strat[k+'_OOS_Returns'] - rf['2000':'2011']['US3M'] <0])\n",
    "    perf_summary['Total Months'] = len(strat)\n",
    "    perf_summary.index = [k]\n",
    "    reg = time_series_regression(strat, factor[strat.index[0]:].squeeze(), False)\n",
    "    perf_summary['Market Beta'] = reg['SPY beta'].values\n",
    "    perf_summary['Market Alpha'] = reg['alpha_hat'].values\n",
    "    perf_summary['Market Information Ratio'] = reg['info_ratio'].values\n",
    "    strat_summary.append(perf_summary)\n",
    "    \n",
    "\n",
    "strat_summary_df_0011 = pd.concat(strat_summary)\n",
    "strat_summary_df_0011.loc[:,['Mean','Volatility','Sharpe Ratio','VaR (0.05)','Max Drawdown','Market Beta','Market Alpha','Market Information Ratio']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd9924",
   "metadata": {},
   "source": [
    "# Rolling & Expanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ef2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "expending_window = np.sqrt((rets**2).expanding(60).mean().shift())\n",
    "rolling_window = np.sqrt((rets**2).rolling(60).mean().shift())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85d59c",
   "metadata": {},
   "source": [
    "# Rolling OOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_regression_param(factor,fund_ret,roll_window = 60):\n",
    "    \"\"\" \n",
    "        Returns the Rolling Regression parameters for given set of returns and factors\n",
    "        Inputs:\n",
    "            factor - Dataframe containing monthly returns of the regressors\n",
    "            fund_ret - Dataframe containing monthly excess returns of the regressand fund\n",
    "            roll_window = rolling window for regression\n",
    "        Output:\n",
    "            params - Dataframe with time-t as the index and constant and Betas as columns\n",
    "    \"\"\"\n",
    "    X = sm.add_constant(factor)\n",
    "    y= fund_ret\n",
    "    rols = RollingOLS(y, X, window=roll_window)\n",
    "    rres = rols.fit()\n",
    "    params = rres.params.copy()\n",
    "    params.index = np.arange(1, params.shape[0] + 1)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_model = RollingOLS(hfri, sm.add_constant(merrill), window=60).fit()  #rolling \n",
    "rolling_params = rolling_model.params\n",
    "factor_ret = merrill.loc['2016-08-31':]    #60 month 说明从5年后开始\n",
    "one_df = pd.DataFrame(np.ones(factor_ret.shape[0])).T\n",
    "one_df.columns = factor_ret.index\n",
    "factor_ret = pd.concat([one_df, pd.DataFrame(factor_ret).loc['2016-08-31':].T])  #第一行都是1\n",
    "\n",
    "rolling_reg_replication = pd.DataFrame(np.diag(pd.DataFrame(np.array(\n",
    "    rolling_params[60:]) @ np.array(factor_ret))), index=factor_ret.columns, columns=['HFRIFWI Index'])\n",
    "performance_summary(rolling_reg_replication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3235fd3",
   "metadata": {},
   "source": [
    "# Forecast/ Expected value/ Fitted Mean based on regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_const = np.array(merrill) @ np.array(coefficients.loc[merrill.columns])\n",
    "without_const = np.array(\n",
    "    merrill) @ np.array(coefficients_new.loc[merrill.columns])\n",
    "no_constant_replication = without_const.mean() * 12\n",
    "original = hedge_fund['HFRIFWI Index'].mean() * 12\n",
    "pd.DataFrame([[no_constant_replication, original]], columns=[\n",
    "             'Replication', 'Actual'], index=['Fitted & Actual Mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf902bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_int = sm.OLS(hfri, sm.add_constant(mer_factors), missing=\"drop\").fit()\n",
    "regr_no_int = sm.OLS(hfri, mer_factors, missing=\"drop\").fit()\n",
    "\n",
    "# Calculate the fitted mean\n",
    "fitted_mean = (regr_int.params[0] + regr_int.params[1:] @ mer_factors.mean()) * 12\n",
    "fitted_mean_no_int = (regr_no_int.params @ mer_factors.mean()) * 12\n",
    "\n",
    "# Get hfri mean\n",
    "hfri_mean = hfri.mean()  * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f881a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_return = (dp_forecast['DP Beta'] * signals['DP'].shift(1).to_frame() + dp_forecast['Alpha']/12)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d161fb6",
   "metadata": {},
   "source": [
    "# subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two sample for momentum\n",
    "BU = size_sorts_tot['BIG HiPRIOR']\n",
    "SU = size_sorts_tot['SMALL HiPRIOR']\n",
    "BD = size_sorts_tot['BIG LoPRIOR']\n",
    "SD = size_sorts_tot['SMALL LoPRIOR']\n",
    "RF = rf_rate['RF']\n",
    "ports = (1/2 * (BU + SU) - RF).to_frame('LONG ONLY')\n",
    "ports['LONG AND SHORT'] = 1/2 * ((BU + SU) - (BD + SD))\n",
    "period = ('1994', '2023')\n",
    "mom_perf = []\n",
    "\n",
    "for port in ports.columns: \n",
    "    sub_mom = ports[[port]].loc[period[0]:period[1]]\n",
    "    summary = performance_summary(sub_mom)\n",
    "    sub_factors = factors_ex.loc[period[0]: period[1]]\n",
    "    summary['Corr Mkt'] = np.corrcoef(np.array(sub_factors['MKT']), np.array(sub_mom[port]))[0,1]\n",
    "    summary['Corr Val'] = np.corrcoef(np.array(sub_factors['HML']), np.array(sub_mom[port]))[0,1]\n",
    "    summary['Corr Size'] = np.corrcoef(np.array(sub_factors['SMB']), np.array(sub_mom[port]))[0,1]\n",
    "    mom_perf.append(summary)\n",
    "\n",
    "mom_perf = pd.concat(mom_perf)\n",
    "display(mom_perf[['Mean', 'Volatility', 'Sharpe Ratio', 'Skewness', 'Corr Mkt', 'Corr Val']].style.format('{:.4f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log level\n",
    "agg_years = [(1965, 1999), (2000, 2023), (1926, 2023)]\n",
    "def calc_analytics_by_year(data, agg_years):\n",
    "    res = []\n",
    "    for y in agg_years:\n",
    "        sub = data.loc[str(y[0]):str(y[1])]\n",
    "        res.append({'mean': sub.mean() * 12, 'vol': sub.std() * np.sqrt(12)})\n",
    "    return pd.DataFrame(res, index = [f'{i[0]} - {i[1]}' for i in agg_years]).stack()\n",
    "\n",
    "sum_stats = data.apply(calc_analytics_by_year, agg_years = agg_years).T\n",
    "log_sum_stats = np.log(1 + data).apply(calc_analytics_by_year, agg_years = agg_years).T\n",
    "\n",
    "res_stats = pd.concat([sum_stats, log_sum_stats])\n",
    "res_stats.index = pd.MultiIndex.from_product([['levels','logs'], sum_stats.index.to_list()])\n",
    "res_stats.style.format('{:,.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log level\n",
    "stats_table = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_tuples([('levels', 'SPX'), ('levels', 'RiskFree'), (\n",
    "        'levels', 'Excess SPX'), ('logs', 'SPX'), ('logs', 'RiskFree'), ('logs', 'Excess SPX')]),\n",
    "    columns=pd.MultiIndex.from_tuples([('1965-1999', 'mean'), ('1965-1999', 'vol'), ('2000-2023', 'mean'), ('2000-2023', 'vol'), ('1926-2023', 'mean'), ('1926-2023', 'vol')]))\n",
    "\n",
    "\n",
    "stats_table[('1965-1999', 'mean')] = ((df.loc['1965':'1999']).mean()* 12).tolist() + ((np.log(1+df.loc['1965':'1999'])).mean()*12).tolist()\n",
    "stats_table[('1965-1999', 'vol')] = ((df.loc['1965':'1999']).std() * np.sqrt(12)).tolist() + ((np.log(1+df.loc['1965':'1999'])).std() * np.sqrt(12)).tolist()\n",
    "stats_table[('2000-2023', 'mean')] = ((df.loc['2000':'2023']).mean()* 12).tolist() + ((np.log(1+df.loc['2000':'2023'])).mean()*12).tolist()\n",
    "stats_table[('2000-2023', 'vol')] = ((df.loc['2000':'2023']).std() * np.sqrt(12)).tolist() + ((np.log(1+df.loc['2000':'2023'])).std() * np.sqrt(12)).tolist()\n",
    "stats_table[('1926-2023', 'mean')] = (np.mean(df) * 12).tolist() + (np.mean(np.log(1+df))*12).tolist()\n",
    "stats_table[('1926-2023', 'vol')] = (df.std()* np.sqrt(12)).tolist() + ((np.log(1+df)).std() * np.sqrt(12)).tolist()\n",
    "\n",
    "display(stats_table.style.format('{:,.2%}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c13826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#different time \n",
    "sub_1980 = factors.loc[:'1980']\n",
    "sub_2001 = factors.loc['1981':'2001']\n",
    "sub_2022 = factors.loc['2002':]\n",
    "\n",
    "df_dict={'1926-1980' : sub_1980,\n",
    "         '1981-2001' : sub_2001,\n",
    "         '2002-2022' : sub_2022}\n",
    "\n",
    "summary_lst = []\n",
    "for key in df_dict.keys():\n",
    "    summary_stats = performance_summary(df_dict[key]).loc[:,['Mean','Volatility','Sharpe Ratio','VaR (0.05)']]\n",
    "    summary_stats['Period'] = key\n",
    "    summary_stats= summary_stats.reset_index().rename(columns = {'index':'Factor'}).set_index(['Period','Factor'])\n",
    "    summary_lst.append(summary_stats)\n",
    "\n",
    "factor_summary = pd.concat(summary_lst)\n",
    "factor_summary\n",
    "\n",
    "\n",
    "\n",
    "#another version\n",
    "sub_samples = {\n",
    "              '1993-2011' : ['1993','2011'],\n",
    "              '2012-2023' : ['2012','2023'],\n",
    "              '1993-2023' : ['1993','2023'],\n",
    "              }\n",
    "\n",
    "gmo_sum = []\n",
    "for k,v in sub_samples.items():\n",
    "    sub_gmo = gmo.loc[sub_samples[k][0]:sub_samples[k][1],['GMWAX']].dropna()\n",
    "    gmo_summary = performance_summary(sub_gmo)\n",
    "    gmo_summary = gmo_summary\n",
    "    gmo_summary.index = [k]\n",
    "    gmo_sum.append(gmo_summary)\n",
    "\n",
    "gmo_summary = pd.concat(gmo_sum)\n",
    "gmo_summary.loc[:,['Mean','Volatility','Sharpe Ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentum different time\n",
    "periods = [['1927', '2022'], ['1927', '1993'], ['1994', '2008'], ['2009', '2023']]\n",
    "summary_col_names = ['Annualized Return','Annualized Volatility','Annualized Sharpe Ratio','Skewness']\n",
    "res = []\n",
    "for period in periods:\n",
    "    temp = momentum.loc[period[0]:period[1]]\n",
    "    temp_ff = ff_factors.loc[period[0]:period[1]]\n",
    "    summary = calc_performance_metrics(temp)[summary_col_names]\n",
    "    summary['mkt_corr'] = temp_ff.corr().loc['MKT',['UMD']]\n",
    "    summary['val_corr'] = temp_ff.corr().loc['HML',['UMD']]\n",
    "    summary = summary.T.iloc[:,0].rename(f'{period[0]} - {period[1]}')\n",
    "    res.append(summary)\n",
    "summary  = pd.concat(res, axis=1).T\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293abd12",
   "metadata": {},
   "source": [
    "# Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830115cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(mu, sigma, h):\n",
    "    return norm.cdf(np.sqrt(h)*mu/sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6053aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "strike = np.log(1.06)\n",
    "mean_return = np.log(1+data['SPX'].loc['1965':]).mean() * 12\n",
    "sigma = np.log(1 + data['SPX'].loc['1965':]).std() * np.sqrt(12)\n",
    "performance = np.log(1+data['SPX'].loc['2000':]).mean() * 12\n",
    "mu = 24/6 * (strike - performance) + strike - mean_return\n",
    "\n",
    "print(f'Probability of Puts ending in the money: {prob(mu, sigma, 6):,.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5ef73",
   "metadata": {},
   "source": [
    "# CVAR VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iid, cdf, norm\n",
    "VaR:  = -1.65 * std\n",
    "cVaR: = 0-norm.pdf(1.65) / 0.05 * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceed_historical = forecasting[forecasting['GLD'] < gld_performance['VaR (0.05)'].values[0]]['GLD'].count()*100/len(forecasting['GLD'])\n",
    "exceed_norm_full = forecasting[forecasting['GLD'] < VaR_full_sample.values[0]]['GLD'].count()*100/len(forecasting['GLD'])\n",
    "exceed_norm_roll = sum(forecasting['GLD'].loc[rolling_var.index] < rolling_var['Rolling VaR'])*100/len(rolling_var['Rolling VaR'])\n",
    "pd.DataFrame([[exceed_historical,exceed_norm_full,exceed_norm_roll]], columns = ['Historical','Normal (Full Sample)','Normal (Rolling)'], index= ['% Data below 5% VaR backtest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a738eba",
   "metadata": {},
   "source": [
    "# Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3076db",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_corr = factors.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "heatmap = sns.heatmap(factor_corr, vmin=0, vmax=1, annot=True)\n",
    "heatmap.set_title('Factors Correlation Heatmap', fontdict={'fontsize':12}, pad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#three heatmap together \n",
    "fig, (ax,ax2,ax3) = plt.subplots(ncols=3)\n",
    "\n",
    "# fig.subplots_adjust(wspace=0.01)\n",
    "# fig.tight_layout(pad=1)\n",
    "sns.heatmap(sub_1980.corr(), ax=ax, cbar=False, annot = True).set_title('1926 - 1980', fontdict={'fontsize':12}, pad=12)\n",
    "\n",
    "fig.colorbar(ax.collections[0], ax=ax,location=\"left\", use_gridspec=False, pad=0.2)\n",
    "sns.heatmap(sub_2001.corr(), ax=ax2, cbar=False, annot = True).set_title('1981 - 2001', fontdict={'fontsize':12}, pad=12)\n",
    "\n",
    "fig.colorbar(ax.collections[0],ax=ax2,location=\"right\", use_gridspec=False, pad=0.2)\n",
    "\n",
    "sns.heatmap(sub_2023.corr(), ax=ax3, cbar=False, annot = True).set_title('2002 - 2023', fontdict={'fontsize':12}, pad=12)\n",
    "fig.colorbar(ax.collections[0],ax=ax3,location=\"right\", use_gridspec=False, pad=0.2)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns of factors, add vertical line at 2015\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "cum_rets = (1 + factors).cumprod() - 1\n",
    "cum_rets.plot(ax=ax)\n",
    "ax.axvline('2015', color='k', linestyle='--')\n",
    "ax.set_title('Cumulative Returns of Factors')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.set_xlabel('Date');\n",
    "\n",
    "\n",
    "\n",
    "plt.plot((1 + ports['1994':]).cumprod(), label = ports.columns)\n",
    "plt.title('Cumulative Return since 1994')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f5696c",
   "metadata": {},
   "source": [
    "# Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cumulative return figures\n",
    "figure = ((factors + 1).cumprod()).plot()\n",
    "# plt.figure(figsize=(100, 6))\n",
    "plt.title('Cumulative Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two cumulative return figures\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Sub Period Cumulative Returns')\n",
    "ax1.plot(((sub_2001 + 1).cumprod()*100)-100)\n",
    "ax2.plot(((sub_2023 + 1).cumprod()*100)-100)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "ax1.legend(sub_2001.columns)\n",
    "ax2.legend(sub_2023.columns)\n",
    "\n",
    "ax1.title.set_text('1981-2001')\n",
    "ax2.title.set_text('2002-2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "plt.scatter(portfolio_summary.loc[:,['Volatility']],portfolio_summary.loc[:,['Mean']])\n",
    "plt.xlabel(\"Volatility\")\n",
    "plt.ylabel(\"Mean Excess Returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba887d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6f532ef",
   "metadata": {},
   "source": [
    "## Dynamic Carry Trade\n",
    "Use this to write $\\mathbb{E}_{t}[\\tilde{r}^{i}_{t+1}]$ as a function of the interest-rate differential as well as $\\alpha$ and $\\beta$ from this FX regression.<br>\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{t}[{s}_{t+1} - {s}_{t}] = \\alpha + \\beta({r}^{f,\\$}_{t,t+1} -  {r}^{f,i}_{t,t+1})\n",
    "\\end{align}\n",
    "\n",
    "Then use the definition of excess (log) returns on FX:<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{r}^{i}_{t+1} = {s}_{t+1} - {s}_{t} - ({r}^{f,\\$}_{t,t+1} -  {r}^{f,i}_{t,t+1})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Rearranging, this implies the following forecast for excess log returns:<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_[\\tilde{r}^{i}_{t+1}] = \\alpha + (\\beta-1) ({r}^{f,\\$}_{t,t+1} -  {r}^{f,i}_{t,t+1})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378ed72",
   "metadata": {},
   "source": [
    "### 3.4.a) Use your regression estimates from Problem 3 along with the formula above to calculate the fraction of months for which the estimated FX risk premium positive. That is, for each i, calculate how often in the time-series we have <br>\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{t}[\\tilde{r}^{i}_{t+1}] > 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3c23e",
   "metadata": {},
   "source": [
    "# Positive Premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8bbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_prem_lst = []\n",
    "for k,v in fx_spot_map.items():\n",
    "    fx_er_usd = (risk_free_rates['log_USD1M'].shift(1) - risk_free_rates[k].shift(1)).to_frame('ER_over_USD')\n",
    "    expected_fx_premium = float(fx_regression_results.loc[[k[4:7]], 'Alpha'])/12 + (fx_er_usd.loc[:,['ER_over_USD']]  * float(fx_regression_results.loc[[k[4:7]], 'Beta'] - 1))\n",
    "    expected_fx_premium = expected_fx_premium.rename(columns={'ER_over_USD':k[4:7]})\n",
    "    positive_premium =  len(expected_fx_premium[expected_fx_premium[k[4:7]] > 0])\n",
    "    fx_prem_lst.append(pd.DataFrame([[positive_premium,len(expected_fx_premium),positive_premium*100/len(expected_fx_premium)]],columns=['Months - Positive Premium','Total Months','Frequency(%)-Positive Premium'],index=[k[4:7]]))\n",
    "fx_premium = pd.concat(fx_prem_lst)\n",
    "fx_premium\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a099288",
   "metadata": {},
   "source": [
    "# Negative premium month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "strats = {'DP': dp_return.dropna(),\n",
    "          'EP': ep_return.dropna(),\n",
    "          'DP-EP-US10Y': factors_return.dropna()\n",
    "         }\n",
    "factor = gmo.loc[:,['SPY']]\n",
    "strat_summary =[]\n",
    "for k,v in strats.items():\n",
    "    strat = strats[k]\n",
    "    perf_summary = performance_summary(strat)\n",
    "    perf_summary['Negative Risk Premium Months'] = len(strat[strat['Forecasted Return'] - rf['US3M'] <0])\n",
    "    perf_summary['Total Months'] = len(strat)\n",
    "    perf_summary.index = [k]\n",
    "    # X is SPY, y is returns predicted by signals\n",
    "    reg = regression_based_performance(factor[strat.index[0]:],strat)\n",
    "    perf_summary['Market Beta'] = reg[0][0]\n",
    "    perf_summary['Market Alpha'] = reg[3]\n",
    "    perf_summary['Market Information Ratio'] = reg[2]\n",
    "    strat_summary.append(perf_summary)\n",
    "    \n",
    "\n",
    "strat_summary_df = pd.concat(strat_summary)\n",
    "strat_summary_df.loc[:,['Mean','Volatility','Sharpe Ratio','Max Drawdown','Market Beta','Market Alpha','Market Information Ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd06c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
